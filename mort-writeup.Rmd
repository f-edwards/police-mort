---
title: "Race, place, and deaths involving police"
author: "Frank and Mike"
date: "May 17, 2017"
output: pdf_document
---
```{r setup, echo=FALSE, asis=TRUE, message=FALSE, warning=FALSE}
library(MASS)

library(ggplot2)
library(readr)
library(date)
library(rstanarm)
library(dplyr)
library(tidyr)
library(xtable)
load("models.RData")

```

# Abstract
*Objective.* To estimate mortality for police-involved deaths by race and geography in the United States.

*Methods.* We estimate mortality rates for deaths involving police by race and by census region using data on all police-involved deaths between 2013 and 2017. We estimate multilevel Bayesian Poisson models that adjust for unobserved geographic characteristics and allow for more predictive precision for rare events, such as law enforcement deaths.

*Results.* Summarize mortality estimates. Summarize fit on observed. Summarize variation across places, races, metros. 

*Conclusions.* Abrupt statement

*Policy Implications.* Efforts to address should take race, place seriously. Clear mort risk for PoC, treat as pub health problem

# Introduction

Violent and fatal interactions between law enforcement and people of color are a persistent feature of American social life [CITATIONS]. In spite of the well-documented shortcomings of federal efforts to systematically document deaths involving police [CITATIONS], newspapers and journalists have undertaken a series of systematic efforts to provide comprehensive data on fatalities resulting from interactions between police and the public [CITATIONS]. This analysis exploits these new data to provide estimates of mortality risk by race and geography. 

Race, place, and policing are important social determinants of health [CITATIONS]. Prior research has clearly established that African Americans are at higher risk of death in interactions with law enforcement [] and has demonstrated clear links between race, place and health outcomes []. We contribute to this literature by illustrating how the risk of death in encounters with law enforcement systematically varies across places by race. In doing so, we illustrate how geography and race interact to produce distinctly higher risks for mortality in encounters with law enforcement for people of color.
Using data from the *Fatal Encounters* database of deaths resulting from law enforcement action, we estimate mortality risks for Black, Latino, and white populations by census division and by metro type. 

# Background

What we know about prevalence of police shootings, likely sources of place variation

Concise lit review with no more than 35 references

WHY WE HAVE SHITTY DATA

# Data and Methods

Data on police-involved deaths come from *Fatal Encounters*, is an effort to document episodes of fatal police-civilian interactions [CITATION]. The project relies on contributions of professional and volunteer researchers, compiled from media reports and public records. The universe of cases in *Fatal Encounters* is broader than similar projects, such as *The Washington Post's* compilation of data on police shootings, has a greater temporal coverage than *The Guardian's* dataset on police-involved deaths, and has a greater geographic coverage than CDC MORT SYSTEM. In a recent report, the Bureau of Justice Statistics noted that the universe of cases included in *Fatal Encounters* provides the most comprehensive treatment of police-involved deaths, and closely matches the scope of the BJS's own *Arrest Related Deaths* data series. BJS has reported that the inclusion of data from this and other sources provides more reliable estimates of officer-involved mortality than does current federal data collection, which relies on voluntary reporting from law enforcement agencies. We calculate county-level mortality rates by race and ethnicity for the period between January 1, 2013 and May 8, 2017. We rely on data from the American Community Survey 5-year 2009 - 2014 population estimates for rate denominators of population by race, ethnicity and county [RUGGLES]. For geographic classification, we rely on the U.S. Census Bureau's 2010 state division classification (see Table X), and use the National Center for Health Statistics' 6 category urban-rural county classification scheme for all US counties.

Because deaths in police custody are a relatively rare event ADD R TOTAL INCIDENCE RATE HERE, we construct regression models to pool power across data to provide predictions of mortality rates by race, by metropolitan status, and by region. These figures provide more reliable estimates of police mortality rates by adjusting for the rarity of this event in small population places. Race data are not completely reported in *Fatal Encounters*, because they are often excluded from news reports or public records. We use victim names and county of residence to predict victim race [@imai_improving_2016], only assigning race in cases with a 75 percent or higher posterior probability of membership, and leaving those cases with less than a 75 percent posterior probability of membership in any category unidentified (X percent of cases).

We estimate multilevel Poisson regressions of police-involved deaths as a function of the race of the victim, metropolitan status, and region. We then estimate posterior predictive mortality rates for each subset of race by metropolitan status by region. While frequentist inferences may be appropriate for counties with large populations, they greatly distort estimates from places with small populations, where any incident can dramatically effect estimates of per capita rates. Bayesian predictive intervals provide more realistic estimates of population mortality rates, because they average over the instabilities that may occur as a result of idiosyncratic local or annual trends through large numbers of simulations. 

Why Bayes and regression? Tons of zeroes - these aren't actual indicators of zero risk, just zero incidence in the window for which we have data. This approach lets us estimate plausible levels of population risk for those counties that have had exceedingly high rates due to small denominators and for counties with zeroes. Risk is unlikely to be true zero. 

Models:

EXPLAIN PARAMETERIZATION, EXPLAIN PRIOR FOR b_0

$$log(E(y|X)) = \beta_0 + \beta_1x + \gamma + \varepsilon + log(offset)$$
$$\beta_0 \sim Normal(log(\alpha) - log(offset), 10) $$
$$ \beta_1 \sim Normal (0, 2.5) $$
$$ \gamma, \varepsilon \sim MVN(0, \Sigma) $$
$$ \Sigma \sim decov(1, 1, 1, 1) $$

# Findings

East North Central: WI, MI, IL, IN, OH

East South Central: KY, TN, AL, MS

Middle Atlantic: NY, PA, NJ

Mountain: MT, ID, WY, NV, UT, CO, AZ, NM

New England: ME, VT, NH, MA, CT, RI

Pacific: AK, HI, WA, OR, CA

South Atlantic: DE, MD, WV, VA, NC, SC, GA, FL

West North Central: ND, SD, NE, KS, MN, IA, MO

West South Central: OK, TX, AR, LA

Big takeaway: the Bayesian predictive estimates smooth out the extremes in estimates from small population counties. They also provide more reasonably estimates for the counties with zero observed estimates. For reasonable estimates of population risk, these posterior estimates are superior to the frequentist alternative. 

African Americans at much higher risk of mortality than are whites, risk is generally highest in medium metros, Pacific states. Latino risk also generally higher than white risk, very sensitive to division, highest in non-large metro counties, highest in non-core. 

```{r, echo=FALSE, message=FALSE}

################################
########## Analysis ############
################################

# 1: EDA
# just pulling fips's over 1000 of each
# tmp = tmp1 %>%
# 	  filter(black >= 1000, white >= 1000)
# 
# # keep most observations, but still
# sum(tmp$d.black)
# sum(tmp$d.white)
# 
# # ... distrbution of rates 
# summary(tmp$y.black)
# summary(tmp$y.white)
# 
# # ... distrbution of counts
# # overwhelmingly 1-2 per county
# summary(tmp$d.black)
# summary(tmp$d.white)
# 
# plot(table(tmp$d.black))
# plot(table(tmp$d.white))
# 
# # ... rate var by county
# dat1 = tmp %>%
# 	   gather(y.race, y.rate, y.black: y.amind) %>%
# 	   select(fips, y.race, y.rate, ur.code, division) %>%
# 	   filter(y.race %in% c('y.black', 'y.white'))
# 
# ggplot(dat1, aes(x = fips, y = log(y.rate), color = y.race)) +
# 	geom_jitter(alpha = .35)  +
# 	theme_bw() +
# 	scale_color_brewer(palette = 'Set2') +
# 	coord_flip() +
# 	ylab('(log) Rate Killed')

# ... average rate by urban/rural type
dat2 = tmp1 %>%
	   group_by(ur.code) %>% 
	   summarise(y.black.mean  = sum(d.black)/sum(black)*100000,
	             y.latino.mean = sum(d.latino)/sum(latino)*100000,
	  			     y.white.mean  = sum(d.white)/sum(white)*100000) %>%
	   gather(rate, value, y.black.mean:y.white.mean )

ggplot(dat2, aes(x = ur.code, y = value, color = rate, group = rate)) +
	geom_point() +
	geom_line() +
	theme_bw() +
	scale_color_brewer(palette = 'Set2') +
	xlab('CDC Code') +
	ylab('Average Rate Killed, per 100,000')+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))+
  ggtitle("Average rate of mortality from interactions with law\nenforcement by race/ethnicity, and metro type")


# ... average rate by division 
dat3 = tmp1 %>%
	   group_by(division) %>% 
	   summarise(y.black.mean  = sum(d.black)/sum(black)*100000,
	             y.latino.mean = sum(d.latino)/sum(latino)*100000,
	  			     y.white.mean  = sum(d.white)/sum(white)*100000) %>%
	   gather(rate, value, y.black.mean:y.white.mean)

ggplot(filter(dat3),
	aes(x = division, y = value, color = rate, group = rate)) +
	geom_point() +
	theme_bw() +
	scale_color_brewer(palette = 'Set2') +
  geom_line()+
	xlab('CDC Code') +
	ylab('Average Rate Killed, per 100,000') +
  theme(axis.text.x = element_text(angle = 75, hjust = 1))+
  ggtitle("Average rate of mortality from interactions with law\nenforcement by race/ethnicity and region")
```

```{r, r, echo=FALSE, fig.height=9, fig.width=7}
# ... average rate by interaction 
dat4 = tmp1 %>%
	   group_by(ur.code, division) %>%
	   summarise(y.black.mean  = sum(d.black)/sum(black)*100000,
	             y.latino.mean = sum(d.latino)/sum(latino)*100000,
	  			     y.white.mean  = sum(d.white)/sum(white)*100000) %>%
	   gather(rate, value, y.black.mean:y.white.mean)

ggplot(filter(dat4),
	   aes(x = ur.code, y = value, group = rate, color = rate)) +
	facet_wrap(~division) + 
	geom_point(size = 2) +
	geom_line() +
	#coord_flip() +
	theme_bw() +
	scale_color_brewer(palette = 'Set2') +
  coord_cartesian(ylim=c(0, 20))+
	xlab('CDC Code') +
	ylab('Rate Killed, per 100,000') +
	theme(axis.text.x = element_text(angle = 75, hjust = 1, size = 9))+
  ggtitle("Average rate of mortality from interactions with law\nenforcement by race/ethnicity, and metro type")



```

Bayesian estimates

```{r, echo=FALSE, message=FALSE, fig.height=9, fig.width=7}
# z<-posterior_predict(blk.stan, newdata=tmp1)
# 
# ### plot mean ppd vs observed, looks alright
# plot(tmp1$d.black, apply(z, 2, mean))
# abline(0, 1)
# 
# prop_zero <- function(y) mean(y == 0)
# prop_zero_test <- pp_check(blk.stan, plotfun = "stat", stat = "prop_zero")

#### do ppd checks for each race/region/ur combo with 100k pop
#### obtain pp intervals for each combo
#### obtain diff/ratio of intervals for each combo
ur.code<-unique(tmp2$ur.code)
division<-unique(tmp2$division)
ur.division<-expand.grid(division, ur.code); names(ur.division)<-c("division", "ur.code")
n<-length(division)*length(ur.code)
### set up newdata for ppd - use 100,000,000 for offset to get enough trials, then rescale to rate per 100,000
newdata<-tmp2[1:n,]
newdata$fips<-0; newdata$tot.pop<-newdata$black<-newdata$white<-newdata$latino<-100000000; 
newdata$ur.code<-ur.division$ur.code; newdata$division<-ur.division$division

# post.all<-posterior_predict(all.stan, newdata=newdata)/1000
# post.all.int<-bind_cols(ur.division, 
#                         as.data.frame(t(apply(post.all, 2, function(x)quantile(x, probs=c(0.025, 0.5, 0.975))))))

post.blk<-posterior_predict(blk.stan, newdata=newdata)/1000
post.blk.int<-bind_cols(ur.division, 
                        as.data.frame(t(apply(post.blk, 2, function(x)quantile(x, probs=c(0.025, 0.5, 0.975))))))

post.wht<-posterior_predict(wht.stan, newdata=newdata)/1000
post.wht.int<-bind_cols(ur.division, 
                        as.data.frame(t(apply(post.wht, 2, function(x)quantile(x, probs=c(0.025, 0.5, 0.975))))))

post.lat<-posterior_predict(lat.stan, newdata=newdata)/1000
post.lat.int<-bind_cols(ur.division,
                        as.data.frame(t(apply(post.lat, 2, function(x)quantile(x, probs=c(0.025, 0.5, 0.975))))))

# plot.dat<-bind_cols(ur.division, data.frame(value=apply(post.all, 2, median)))
# plot.dat$rate<-"All"

plot.dat<-bind_cols(ur.division, data.frame(value=apply(post.blk, 2, median)))
plot.dat$rate<-"Black"

plot.tmp<-bind_cols(ur.division, data.frame(value=apply(post.wht, 2, median)))
plot.tmp$rate<-"White"

plot.dat<-bind_rows(plot.dat, plot.tmp)

plot.tmp<-bind_cols(ur.division, data.frame(value=apply(post.lat, 2, median)))
plot.tmp$rate<-"Latino"

plot.dat<-bind_rows(plot.dat, plot.tmp)

plot.dat$ur.code<-factor(plot.dat$ur.code, levels=sort(as.character(unique(plot.dat$ur.code))))
plot.dat$division<-factor(plot.dat$division, levels=sort(as.character(unique(plot.dat$division))))

plot.dat<-plot.dat[order(plot.dat$division, plot.dat$ur.code), ]

ggplot(plot.dat,
       aes(x = ur.code, y = value, group = rate, color = rate)) +
  facet_wrap(~division) + 
  geom_point(size = 2) +
  geom_line() +
  #coord_flip() +
  theme_bw() +
  scale_color_brewer(palette = 'Set2') +
  xlab('CDC Code') +
  ylab('Rate Killed, per 100,000') + 
  ylim(0, 15)+
  theme(axis.text.x = element_text(angle = 75, hjust = 1, size = 10))+
  ggtitle("Posterior average rate of mortality from interactions with law\nenforcement by race/ethnicity, and metro type")

########### get posterior rate ratios
bw.post<-bind_cols(ur.division, 
                   as.data.frame(t(apply(post.blk - post.wht, 2, function(x)quantile(x, probs=c(0.05, 0.5, 0.95))))))
bw.post$rate<-"Black - White"
lw.post<-bind_cols(ur.division, 
                   as.data.frame(t(apply(post.lat - post.wht, 2, function(x)quantile(x, probs=c(0.05, 0.5, 0.95))))))
lw.post$rate<-"Latino - White"

plot.dat<-bind_rows(bw.post, lw.post)

plot.dat$ur.code<-factor(plot.dat$ur.code, levels=sort(as.character(unique(plot.dat$ur.code))))
plot.dat$division<-factor(plot.dat$division, levels=sort(as.character(unique(plot.dat$division))))

plot.dat<-plot.dat[order(plot.dat$ur.code, plot.dat$division), ]
names(plot.dat)[3:5]<-c("Lower", "Median", "Upper")

ggplot(plot.dat,
  aes(x = ur.code, y = Median, group = rate, color = rate)) +
  facet_wrap(~division) + 
  geom_point(size = 2) +
  geom_line() +
  #geom_errorbar(aes(ymin = Lower, ymax= Upper)) +
  theme_bw() +
  scale_color_brewer(palette = 'Set2') +
  xlab('County Type') +
  ylab('Rate difference') + 
  geom_hline(yintercept=0, lty=2)+
  #coord_cartesian(ylim=c(0, 30))+
  theme(axis.text.x = element_text(angle = 75, hjust = 1, size = 10))+
  ggtitle("Posterior difference in average rate of mortality from interactions with law\nenforcement by race/ethnicity, and metro type")

######################
### Tables
########################

maketable<-function(x, label){
  simpleCap <- function(x) {
    s <- strsplit(x, " ")[[1]]
    paste(toupper(substring(s, 1,1)), substring(s, 2),
          sep="", collapse=" ")
  }
  tab<-x
  names(tab)[3:5]<-c("Lower", "Median", "Upper")
  tab$ur.code<-as.character(tab$ur.code); tab$division<-as.character(tab$division)
  tab<-tab[order(tab$division, tab$ur.code), ]
  tab$ur.code<-substr(tab$ur.code, 4, nchar(tab$ur.code))
  for(i in 1:nrow(tab)){
    tab$ur.code[i]<-simpleCap(tab$ur.code[i])
    tab$ur.code[i]<-paste(" -", tab$ur.code[i])
  }
  tab[, 3:5]<-round(tab[, 3:5], 2)
  tab<-tab%>%mutate(value=paste(Median, " (", Lower, ", ", Upper, ")", sep=""))%>%
    select(-Median, -Lower, -Upper)
  div<-unique(tab$division)
  for(i in 1:length(div)){
    index<-min(which(tab$division==div[i]))
    if(index==1){
      tab<-rbind(c(NA, div[i], NA), tab[index:nrow(tab),])
      }
    if(index!=1){
      tab<-rbind(tab[1:(index-1),], c(NA, div[i], NA), tab[index:nrow(tab),])
      }
  }
  tab<-tab%>%select(-division)
  names(tab)<-c("County Type", label)
  return(tab)
}


tab.out<-cbind(maketable(post.blk.int, "Rate-Black"),
               maketable(post.lat.int, "Rate-Latino")[,2],
               maketable(post.wht.int, "Rate-White")[,2])
names(tab.out)<-c("County Name", "Black", "Latino", "White")
```

Here is a ridiculous table with posterior estimates of mortality by race, region and metro type.

```{r, results="asis", message=FALSE, echo=FALSE}

print(xtable(tab.out, 
             caption="Posterior police related mortality estimates by race/ethnicity, census region, and metro type, 95 percent credible intervals"), 
      include.rownames=FALSE,
      size="\\fontsize{9pt}{10pt}\\selectfont", 
      caption.placement="top",
      comment=FALSE
      )

```
